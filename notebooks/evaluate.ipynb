{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"..\"\n",
    "MODEL_ID = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "MAX_NEW_TOKENS = 8192\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from datasets import Dataset, DatasetDict  # type: ignore\n",
    "\n",
    "from groq import Groq  # type: ignore\n",
    "from dotenv import load_dotenv # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"compare_predictions\": \"\"\"Compare the following prediction with the actual answer:\n",
    "\n",
    "Prediction: {pred}\n",
    "Actual Answer: {actual}\n",
    "\n",
    "Evaluate the prediction's accuracy and provide a brief explanation. \n",
    "Rate the prediction on a scale of 1-5, where 1 is completely incorrect and 5 is perfectly accurate.\n",
    "\n",
    "Response format:\n",
    "{{\n",
    "    \"rating\": [1-5],\n",
    "    \"explanation\": [Your explanation here]\n",
    "}}\n",
    "\"\"\",\n",
    "'compare_answers': \"\"\"Compare the following two answers:\n",
    "\n",
    "Answer 1: {answer1}\n",
    "Answer 2: {answer2}\n",
    "\n",
    "Evaluate the accuracy and provide a brief explanation. \n",
    "Rate the answer on a scale of 1-5, where 1 is completely incorrect and 5 is perfectly accurate.\n",
    "\n",
    "Response format:\n",
    "{{\n",
    "    \"rating\": [1-5],\n",
    "    \"explanation\": [Your explanation here]\n",
    "}}\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from variables.env file\n",
    "load_dotenv(f\"{BASE_PATH}/variables.env\")\n",
    "\n",
    "# Access the GROQ_API_KEY\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Verify that the key was loaded\n",
    "if groq_api_key:\n",
    "    print(\"GROQ API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load GROQ API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def to_dataset(data):\n",
    "    restructured_data = {\n",
    "        \"question\": [],\n",
    "        \"resources\": [],\n",
    "        \"answer\": [],\n",
    "    }\n",
    "\n",
    "    for qna in data:\n",
    "        restructured_data[\"question\"].append(qna[\"question\"])\n",
    "        if \"text\" in qna:\n",
    "            restructured_data[\"answer\"].append(qna[\"text\"])\n",
    "        else:\n",
    "            restructured_data[\"answer\"].append(qna[\"answer\"])\n",
    "        restructured_data[\"resources\"].append('\\n'.join([resource['summary'] for resource in qna[\"citation\"]]))\n",
    "\n",
    "    return Dataset.from_dict(restructured_data)\n",
    "\n",
    "\n",
    "def prepare_dataset(base_path=None):\n",
    "    test_cars = load_data(f\"{base_path}/data/test_qa_car.json\")\n",
    "    test_sleep = load_data(f\"{base_path}/data/test_qa_sleep.json\")\n",
    "    \n",
    "    cars_predictions = load_data(f\"{base_path}/data/cars_predictions.json\")\n",
    "    sleep_predictions = load_data(f\"{base_path}/data/sleep_predictions.json\")\n",
    "\n",
    "    test_cars_dataset = to_dataset(test_cars)\n",
    "    test_sleep_dataset = to_dataset(test_sleep)\n",
    "    \n",
    "    cars_predictions_dataset = to_dataset(cars_predictions)\n",
    "    sleep_predictions_dataset = to_dataset(sleep_predictions)\n",
    "    \n",
    "    return {\"cars\": test_cars_dataset, \"sleep\": test_sleep_dataset, \"cars_predictions\": cars_predictions_dataset, \"sleep_predictions\": sleep_predictions_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make 3 comparisons\n",
    "# 1 - when we tell model which response is ground truth\n",
    "# 2 - when we don't tell it which response is ground truth just ask to compare\n",
    "# 3 - when we do same as previous but swap predicted and ground truth places\n",
    "def compare_predictions(predictions, actual_answers, task_type=\"compare_predictions\"):\n",
    "    results = []\n",
    "    for pred, actual in zip(predictions, actual_answers):\n",
    "        prompt = PROMPTS[task_type].format(pred=pred, actual=actual)\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0.5,\n",
    "            max_tokens=256,\n",
    "        )\n",
    "\n",
    "        results.append(response.choices[0].message.content)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results = compare_predictions(results[\"predictions\"], results[\"answers\"])\n",
    "\n",
    "# Print or process the results as needed\n",
    "for i, result in enumerate(comparison_results):\n",
    "    print(f\"Comparison {i + 1}:\")\n",
    "    print(result)\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
