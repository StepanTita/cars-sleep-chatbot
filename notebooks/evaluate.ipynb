{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"..\"\n",
    "MODEL_ID = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "MAX_NEW_TOKENS = 8192\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from datasets import Dataset, DatasetDict  # type: ignore\n",
    "\n",
    "from groq import Groq  # type: ignore\n",
    "from dotenv import load_dotenv # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from variables.env file\n",
    "load_dotenv(f\"{BASE_PATH}/variables.env\")\n",
    "\n",
    "# Access the GROQ_API_KEY\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Verify that the key was loaded\n",
    "if groq_api_key:\n",
    "    print(\"GROQ API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load GROQ API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make 3 comparisons\n",
    "# 1 - when we tell model which response is ground truth\n",
    "# 2 - when we don't tell it which response is ground truth just ask to compare\n",
    "# 3 - when we do same as previous but swap predicted and ground truth places\n",
    "def compare_predictions(predictions, actual_answers):\n",
    "    results = []\n",
    "    for pred, actual in zip(predictions, actual_answers):\n",
    "        prompt = f\"\"\"Compare the following prediction with the actual answer:\n",
    "\n",
    "Prediction: {pred}\n",
    "Actual Answer: {actual}\n",
    "\n",
    "Evaluate the prediction's accuracy and provide a brief explanation. \n",
    "Rate the prediction on a scale of 1-5, where 1 is completely incorrect and 5 is perfectly accurate.\n",
    "\n",
    "Response format:\n",
    "Rating: [1-5]\n",
    "Explanation: [Your explanation here]\n",
    "\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0.5,\n",
    "            max_tokens=150,\n",
    "        )\n",
    "\n",
    "        results.append(response.choices[0].message.content)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results = compare_predictions(results[\"predictions\"], results[\"answers\"])\n",
    "\n",
    "# Print or process the results as needed\n",
    "for i, result in enumerate(comparison_results):\n",
    "    print(f\"Comparison {i + 1}:\")\n",
    "    print(result)\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
