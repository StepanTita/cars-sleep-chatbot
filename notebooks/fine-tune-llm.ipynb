{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'sleep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/cars-sleep-chatbot\"\n",
    "# MODEL_ID = f\"{BASE_PATH}/models/{dataset_type}/llama-3_2-1b-it\"\n",
    "MODEL_ID = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "MAX_NEW_TOKENS = 8192\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "from datasets import DatasetDict, Dataset  # type: ignore\n",
    "\n",
    "from unsloth import FastLanguageModel  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from trl import SFTTrainer  # type: ignore\n",
    "from transformers import TrainingArguments  # type: ignore\n",
    "from unsloth import is_bfloat16_supported  # type: ignore\n",
    "from datasets import Dataset, DatasetDict  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    'cars': {\n",
    "        'system': \"You are an expert in sleep science with in-depth knowledge of sleep physiology, circadian rhythms, sleep disorders, and the impact of sleep on health and cognitive performance. Your task is to generate insightful and varied answers on sleep-related topics. The answers should be diverse in complexity, suitable for learners and experts alike.\",\n",
    "        'basic': \"Human: Generate me an answer to the given question: {question}\\n\\nAssistant:\",\n",
    "        'rag': \"Use resources provided to answer the following question.\\nResources: {resources}\\n\\nHuman: Generate me an answer to the given question: {question}\\n\\nAssistant:\",\n",
    "    },\n",
    "    'sleep': {\n",
    "        'system': \"You are an expert in the history of automobiles with in-depth knowledge of the development of automobiles from the late 19th century to the present day. Your task is to generate insightful and varied answers on automobile history. The answers should be diverse in complexity, suitable for learners and experts alike.\",\n",
    "        'basic': \"Human: Generate me an answer to the given question: {question}\\n\\nAssistant:\",\n",
    "        'rag': \"Use resources provided to answer the following question.\\nResources: {resources}\\n\\nHuman: Generate me an answer to the given question: {question}\\n\\nAssistant:\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(dtype=None, load_in_4bit=True, add_lora=False):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_ID,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    if add_lora:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=3407,\n",
    "            use_rslora=False,\n",
    "            loftq_config=None,\n",
    "        )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(f):\n",
    "    def wrapper(model, tokenizer, *args, **kwargs):\n",
    "        FastLanguageModel.for_training(model)\n",
    "        return f(model, tokenizer, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.1.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.679 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post3 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(add_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def to_dataset(data):\n",
    "    restructured_data = {\n",
    "        \"question\": [],\n",
    "        \"answer\": [],\n",
    "    }\n",
    "\n",
    "    for qna in data:\n",
    "        restructured_data[\"question\"].append(qna[\"question\"])\n",
    "        restructured_data[\"answer\"].append(qna[\"answer\"])\n",
    "\n",
    "    return Dataset.from_dict(restructured_data)\n",
    "\n",
    "\n",
    "def prepare_dataset(tokenizer, base_path=None, final_training=False):\n",
    "    # Load all datasets\n",
    "    training_cars = load_data(f\"{base_path}/data/cars_qa.json\")\n",
    "    training_sleep = load_data(f\"{base_path}/data/sleep_qa.json\")\n",
    "    \n",
    "    test_cars = load_data(f\"{base_path}/data/test_qa_car.json\")\n",
    "    test_sleep = load_data(f\"{base_path}/data/test_qa_sleep.json\")\n",
    "\n",
    "    training_cars_dataset = to_dataset(training_cars)\n",
    "    training_sleep_dataset = to_dataset(training_sleep)\n",
    "    \n",
    "    test_cars_dataset = to_dataset(test_cars)\n",
    "    test_sleep_dataset = to_dataset(test_sleep)\n",
    "\n",
    "    def create_chat(question, answer, dataset_type='cars'):\n",
    "        system_content = PROMPTS[dataset_type]['system']\n",
    "        user_content = PROMPTS[dataset_type]['basic'].format(question=question)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        return messages\n",
    "\n",
    "    def process_dataset(examples, dataset_type):\n",
    "        chats = [create_chat(q, a, dataset_type) for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "        texts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in chats]\n",
    "        return {\"texts\": texts, \"messages\": chats}\n",
    "\n",
    "    cars_train = training_cars_dataset.map(lambda x: process_dataset(x, 'cars'), batched=True)\n",
    "    sleep_train = training_sleep_dataset.map(lambda x: process_dataset(x, 'sleep'), batched=True)\n",
    "    \n",
    "    cars_test = test_cars_dataset.map(lambda x: process_dataset(x, 'cars'), batched=True)\n",
    "    sleep_test = test_sleep_dataset.map(lambda x: process_dataset(x, 'sleep'), batched=True)\n",
    "\n",
    "    if final_training:\n",
    "        cars_dataset = DatasetDict({\n",
    "            \"train\": cars_train,\n",
    "            \"test\": cars_test,\n",
    "        })\n",
    "        sleep_dataset = DatasetDict({\n",
    "            \"train\": sleep_train,\n",
    "            \"test\": sleep_test,\n",
    "        })\n",
    "    else:\n",
    "        cars_train, cars_val = cars_train.train_test_split(test_size=0.3, seed=42).values()\n",
    "        sleep_train, sleep_val = sleep_train.train_test_split(test_size=0.3, seed=42).values()\n",
    "\n",
    "        cars_dataset = DatasetDict({\n",
    "            \"train\": cars_train,\n",
    "            \"val\": cars_val,\n",
    "            \"test\": cars_test,\n",
    "        })\n",
    "        sleep_dataset = DatasetDict({\n",
    "            \"train\": sleep_train,\n",
    "            \"val\": sleep_val,\n",
    "            \"test\": sleep_test,\n",
    "        })\n",
    "\n",
    "    return {\"cars\": cars_dataset, \"sleep\": sleep_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118/118 [00:00<00:00, 13472.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<00:00, 14889.49 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 7357.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 7490.32 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cars': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['question', 'answer', 'texts', 'messages'],\n",
       "         num_rows: 118\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['question', 'answer', 'texts', 'messages'],\n",
       "         num_rows: 26\n",
       "     })\n",
       " }),\n",
       " 'sleep': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['question', 'answer', 'texts', 'messages'],\n",
       "         num_rows: 92\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['question', 'answer', 'texts', 'messages'],\n",
       "         num_rows: 27\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = prepare_dataset(tokenizer, base_path=BASE_PATH, final_training=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@train\n",
    "def training(model, tokenizer, dataset, max_seq_length, dataset_type):\n",
    "    common_args = {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"train_dataset\": dataset[\"train\"],\n",
    "        \"dataset_text_field\": \"texts\",\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"dataset_num_proc\": 2,\n",
    "        \"packing\": False,\n",
    "    }\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        logging_steps=100,\n",
    "        warmup_steps=5,\n",
    "        max_steps=25,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=f\"{BASE_PATH}/models/{dataset_type}/llama-3_2-1b-it\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=250,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    if \"val\" in dataset:\n",
    "        common_args[\"eval_dataset\"] = dataset[\"val\"]\n",
    "        training_args.per_device_eval_batch_size = 1\n",
    "        training_args.eval_strategy = \"steps\"\n",
    "        training_args.eval_steps = 100\n",
    "        training_args.metric_for_best_model = \"eval_loss\"\n",
    "        training_args.save_best_model = True\n",
    "\n",
    "    trainer = SFTTrainer(args=training_args, **common_args)\n",
    "    stats = trainer.train()\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:00<00:00, 97.26 examples/s] \n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 92 | Num Epochs = 19\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 8 | Total steps = 200\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:58, Epoch 17/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.563800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.8048007583618164, metrics={'train_runtime': 180.4266, 'train_samples_per_second': 8.868, 'train_steps_per_second': 1.108, 'total_flos': 5352882249707520.0, 'train_loss': 0.8048007583618164, 'epoch': 17.391304347826086})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer, stats = training(model, tokenizer, dataset[dataset_type], max_seq_length=MAX_SEQ_LENGTH, dataset_type=dataset_type)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{BASE_PATH}/models/{dataset_type}/llama-3_2-1b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: home/stepan/kaggle-arc-agi/models/cars/ (stored 0%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/ (stored 0%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/ (stored 0%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/optimizer.pt (deflated 11%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/training_args.bin (deflated 51%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/adapter_config.json (deflated 54%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/special_tokens_map.json (deflated 71%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/rng_state.pth (deflated 25%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/trainer_state.json (deflated 57%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/tokenizer.json (deflated 85%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/README.md (deflated 66%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/adapter_model.safetensors (deflated 8%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/tokenizer_config.json (deflated 94%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/checkpoint-200/scheduler.pt (deflated 56%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/training_args.bin (deflated 51%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/adapter_config.json (deflated 54%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/special_tokens_map.json (deflated 71%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/tokenizer.json (deflated 85%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/README.md (deflated 66%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/adapter_model.safetensors (deflated 8%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/cars/llama-3_2-1b-it/tokenizer_config.json (deflated 94%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/ (stored 0%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/ (stored 0%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/ (stored 0%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/optimizer.pt (deflated 11%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/training_args.bin (deflated 51%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/adapter_config.json (deflated 54%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/special_tokens_map.json (deflated 71%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/rng_state.pth (deflated 25%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/trainer_state.json (deflated 57%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/tokenizer.json (deflated 85%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/README.md (deflated 66%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/adapter_model.safetensors (deflated 8%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/tokenizer_config.json (deflated 94%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/checkpoint-200/scheduler.pt (deflated 56%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/training_args.bin (deflated 51%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/adapter_config.json (deflated 54%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/special_tokens_map.json (deflated 71%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/tokenizer.json (deflated 85%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/README.md (deflated 66%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/adapter_model.safetensors (deflated 8%)\n",
      "  adding: home/stepan/kaggle-arc-agi/models/sleep/llama-3_2-1b-it/tokenizer_config.json (deflated 94%)\n"
     ]
    }
   ],
   "source": [
    "! zip -r {BASE_PATH}/models/cars.zip {BASE_PATH}/models/cars\n",
    "! zip -r {BASE_PATH}/models/sleep.zip {BASE_PATH}/models/sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
