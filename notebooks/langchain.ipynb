{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/.conda/envs/llm-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import transformers  # type: ignore\n",
    "\n",
    "from tqdm.auto import tqdm  # type: ignore\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline  # type: ignore\n",
    "from langchain.chains import LLMChain  # type: ignore\n",
    "from langchain.prompts import PromptTemplate  # type: ignore\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings  # type: ignore\n",
    "\n",
    "from datasets import Dataset, DatasetDict  # type: ignore\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'sleep'\n",
    "PROMPT_MODE = 'rag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/stepan/kaggle-arc-agi\"\n",
    "MODEL_ID = f\"{BASE_PATH}/models/{dataset_type}/llama-3_2-1b-it\"\n",
    "MAX_NEW_TOKENS = 8192\n",
    "MAX_SEQ_LENGTH = 32768 - MAX_NEW_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    'cars': {\n",
    "        'system': \"You are an expert in sleep science with in-depth knowledge of sleep physiology, circadian rhythms, sleep disorders, and the impact of sleep on health and cognitive performance. Your task is to generate insightful and varied answers on sleep-related topics. The answers should be diverse in complexity, suitable for learners and experts alike.\",\n",
    "        'basic': \"Generate me an answer to the given question: {question}\\n\\n\",\n",
    "        'rag': \"Generate me an answer to the given question: {question}\\n\\nResources: {resources}\",\n",
    "    },\n",
    "    'sleep': {\n",
    "        'system': \"You are an expert in the history of automobiles with in-depth knowledge of the development of automobiles from the late 19th century to the present day. Your task is to generate insightful and varied answers on automobile history. The answers should be diverse in complexity, suitable for learners and experts alike.\",\n",
    "        'basic': \"Generate me an answer to the given question: {question}\\n\\n\",\n",
    "        'rag': \"Generate me an answer to the given question: {question}\\n\\nResources: {resources}\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=MODEL_ID,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.5,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=PROMPTS[dataset_type][PROMPT_MODE],\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llama_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the best car?',\n",
       " 'text': 'Human: Generate me an answer to the given question: What is the best car?\\n\\nAssistant: I\\'d be happy to help you with that. The answer depends on various factors such as budget, personal preferences, and intended use of the vehicle.\\n\\nHere are some top contenders for the \"best\" car:\\n\\n**Luxury Segment**\\n\\n1. **Mercedes-Benz S-Class**: A flagship sedan known for its comfort, performance, and opulent interior.\\n2. **BMW 7-Series**: A premium luxury SUV offering exceptional handling, safety features, and a spacious cabin.\\n3. **Audi R8**: A high-performance sports car featuring cutting-edge technology, stunning design, and thrilling driving experiences.\\n\\n**Performance Segment**\\n\\n1. **Porsche 911**: A legendary sports car renowned for its exceptional handling, acceleration, and distinctive design.\\n2. **Ferrari F8 Tributo**: An ultra-luxurious supercar boasting breathtaking performance, stylish design, and exclusive features.\\n3. **Nissan GT-R**: A high-performance sedan or coupe capable of delivering mind-bending speed and agility.\\n\\n**Family-Friendly Segment**\\n\\n1. **Toyota Avalon**: A spacious, comfortable, and feature-rich full-size sedan ideal for families.\\n2. **Honda Accord**: A reliable, fuel-efficient, and versatile midsize sedan perfect for daily commutes.\\n3. **Subaru Outback**: A rugged, all-wheel-drive SUV suitable for family road trips and outdoor adventures.\\n\\n**Electric/Alternative Segment**\\n\\n1. **Tesla Model S**: An eco-friendly, luxurious electric sedan showcasing innovative technology and seamless driving experiences.\\n2. **Hyundai Kona Electric**: A compact, affordable, and environmentally friendly electric crossover offering impressive range and value.\\n3. **Volkswagen ID.4**: A modern, sustainable electric SUV providing ample space and advanced features at an attractive price point.\\n\\nUltimately, the \"best\" car for you will depend on your specific needs, priorities, and preferences. Consider factors like budget, size, fuel efficiency, and personal style when making your decision. Do any of these options stand out to you?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"question\": \"What is the best car?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def to_dataset(data):\n",
    "    restructured_data = {\n",
    "        \"question\": [],\n",
    "        \"resources\": [],\n",
    "        \"answer\": [],\n",
    "    }\n",
    "\n",
    "    for qna in data:\n",
    "        restructured_data[\"question\"].append(qna[\"question\"])\n",
    "        restructured_data[\"answer\"].append(qna[\"answer\"])\n",
    "        restructured_data[\"resources\"].append('\\n'.join([resource['summary'] for resource in qna[\"citation\"]]))\n",
    "\n",
    "    return Dataset.from_dict(restructured_data)\n",
    "\n",
    "\n",
    "def prepare_dataset(base_path=None):\n",
    "    test_cars = load_data(f\"{base_path}/data/test_qa_car.json\")\n",
    "    test_sleep = load_data(f\"{base_path}/data/test_qa_sleep.json\")\n",
    "\n",
    "    test_cars_dataset = to_dataset(test_cars)\n",
    "    test_sleep_dataset = to_dataset(test_sleep)\n",
    "\n",
    "    return {\"cars\": test_cars_dataset, \"sleep\": test_sleep_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(dataset, dataset_type):\n",
    "    predictions = []\n",
    "    for question in tqdm(dataset[dataset_type][\"question\"]):\n",
    "        predictions.append(llm_chain.invoke({\"question\": question}))\n",
    "    # save predictions\n",
    "    with open(f\"{BASE_PATH}/data/{dataset_type}_predictions.json\", \"w\") as f:\n",
    "        json.dump(predictions, f)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 9/27 [01:56<03:50, 12.83s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 27/27 [06:00<00:00, 13.34s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions(dataset, dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(f\"{BASE_PATH}/data/{dataset_type}.txt\")\n",
    "loader.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
